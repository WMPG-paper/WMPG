{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rollouts_ac(object):\n",
    "    def __init__(self, batch_size, discount):\n",
    "        self.batch_size = batch_size\n",
    "        self.rollout_memory = []\n",
    "        self.rollout_values = []\n",
    "        self.batch = []\n",
    "        self.discount = discount\n",
    "        self.transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'terminal'))\n",
    "\n",
    "    def init_episode(self):\n",
    "        self.trajectory = []\n",
    "        \n",
    "    def push_to_trajectory(self, state, action, next_state, reward, terminal):\n",
    "        self.trajectory.append(self.transition(state, action, next_state, reward, terminal))\n",
    "        \n",
    "    def monte_carlo_unfinished(self, value_network):\n",
    "        values = []\n",
    "        final_state = self.trajectory[-1].next_state\n",
    "        with torch.no_grad():\n",
    "            final_state_val = value_network.forward(final_state)\n",
    "        for idx, state in enumerate(self.trajectory):\n",
    "            value = 0 \n",
    "            for idx_, state_ in enumerate(self.trajectory[idx:]):\n",
    "                value += self.discount**idx_ * state_.reward\n",
    "                if (idx + idx_ + 1) == len(self.trajectory):\n",
    "                    value += self.discount**(idx_+1) * final_state_val\n",
    "            values.append(value)\n",
    "        return values\n",
    "    \n",
    "    def push_to_memory_unfinished(self, value_network):\n",
    "        values = self.monte_carlo_unfinished(value_network)\n",
    "        self.rollout_memory = self.rollout_memory + self.trajectory\n",
    "        self.rollout_values = self.rollout_values + values\n",
    "            \n",
    "    def make_a_batch(self):\n",
    "        batch = self.rollout_memory[:self.batch_size]\n",
    "        batch = self.transition(*zip(*batch))\n",
    "        self.rollout_memory = self.rollout_memory[self.batch_size:]\n",
    "        values = torch.cat(self.rollout_values[:self.batch_size])\n",
    "        self.rollout_values = self.rollout_values[self.batch_size:]\n",
    "        terminal = torch.cat(batch.terminal)\n",
    "        state = torch.cat(batch.state)\n",
    "        action = torch.cat(batch.action)\n",
    "        reward = torch.cat(batch.reward)\n",
    "        new_state = torch.cat(batch.next_state)\n",
    "        return state, action, reward, values, new_state, terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class deterministic_policy(nn.Module):\n",
    "    def __init__(self, STATE_DIM, HIDDEN_LIST, ACTION_DIM):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        prev_layer = STATE_DIM\n",
    "        for layer in HIDDEN_LIST:\n",
    "            self.layers.append(nn.Linear(prev_layer, layer))\n",
    "            self.layers.append(nn.ReLU())\n",
    "            prev_layer = layer\n",
    "        self.layers.append(nn.Linear(prev_layer, ACTION_DIM))                      \n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return F.softmax(x, dim=-1)\n",
    "        \n",
    "        \n",
    "class neural_net(nn.Module):\n",
    "    def __init__(self, STATE_DIM, HIDDEN_LIST, OUTPUT_DIM):\n",
    "        #super(neural_net, self).__init__()\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        prev_layer = STATE_DIM\n",
    "        for layer in HIDDEN_LIST:\n",
    "            self.layers.append(nn.Linear(prev_layer, layer))\n",
    "            self.layers.append(nn.ReLU())\n",
    "            prev_layer = layer\n",
    "        self.layers.append(nn.Linear(prev_layer, OUTPUT_DIM))     \n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class conditioned_transition_net(nn.Module):\n",
    "    def __init__(self, STATE_DIM, HIDDEN_LIST, ACTION_DIM):\n",
    "        #super(conditioned_transition_net, self).__init__()\n",
    "        super().__init__()\n",
    "        self.action_dim = ACTION_DIM\n",
    "        self.layers = nn.ModuleList()\n",
    "        prev_layer = STATE_DIM\n",
    "        for idx, layer in enumerate(HIDDEN_LIST):\n",
    "            if idx == 0:\n",
    "                if ACTION_DIM == 2:\n",
    "                    self.layers.append(nn.Linear(prev_layer+1, layer))\n",
    "                else:\n",
    "                    self.layers.append(nn.Linear(prev_layer+ACTION_DIM, layer))\n",
    "            if idx > 0:\n",
    "                self.layers.append(nn.Linear(prev_layer, layer))\n",
    "            self.layers.append(nn.ReLU())\n",
    "            prev_layer = layer\n",
    "        self.layers.append(nn.Linear(prev_layer, STATE_DIM))   \n",
    "        \n",
    "    def forward(self, x, a):\n",
    "        if self.action_dim == 2:\n",
    "            x = torch.cat((x, a), 1)\n",
    "        else:\n",
    "            extra = torch.zeros([x.size(0), self.action_dim], dtype=torch.float32)\n",
    "            a_ = torch.tensor(a, dtype=torch.int64)\n",
    "            extra = extra.scatter(1,a_,1)\n",
    "            x = torch.cat((x, extra), 1)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class conditioned_q_net(nn.Module):\n",
    "    def __init__(self, STATE_DIM, HIDDEN_LIST, ACTION_DIM):\n",
    "        #super(conditioned_transition_net, self).__init__()\n",
    "        super().__init__()\n",
    "        self.action_dim = ACTION_DIM\n",
    "        self.layers = nn.ModuleList()\n",
    "        prev_layer = STATE_DIM\n",
    "        for idx, layer in enumerate(HIDDEN_LIST):\n",
    "            if idx == 0:\n",
    "                if ACTION_DIM == 2:\n",
    "                    self.layers.append(nn.Linear(prev_layer+1, layer))\n",
    "                else:\n",
    "                    self.layers.append(nn.Linear(prev_layer+ACTION_DIM, layer))\n",
    "            if idx > 0:\n",
    "                self.layers.append(nn.Linear(prev_layer, layer))\n",
    "            self.layers.append(nn.ReLU())\n",
    "            prev_layer = layer\n",
    "        self.layers.append(nn.Linear(prev_layer, 1))   \n",
    "        \n",
    "    def forward(self, x, a):\n",
    "        if self.action_dim == 2:\n",
    "            x = torch.cat((x, a), 1)\n",
    "        else:\n",
    "            extra = torch.zeros([x.size(0), self.action_dim], dtype=torch.float32)\n",
    "            a_ = torch.tensor(a, dtype=torch.int64)\n",
    "            extra = extra.scatter(1,a_,1)\n",
    "            x = torch.cat((x, extra), 1)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "        \n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, state, hidden, latent):\n",
    "        super(VAE, self).__init__()\n",
    "        self.e1 = nn.Linear(state, hidden)\n",
    "        self.e21 = nn.Linear(hidden, latent)\n",
    "        self.e22 = nn.Linear(hidden, latent)\n",
    "        self.d1 = nn.Linear(latent, hidden)\n",
    "        self.d2 = nn.Linear(hidden, state)\n",
    "        self.latent = latent\n",
    "\n",
    "    def encode(self, x):\n",
    "        out = F.relu(self.e1(x))\n",
    "        mu = self.e21(out)\n",
    "        logvar = self.e22(out)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparametrize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        eps = torch.FloatTensor(mu.size()).normal_()\n",
    "        z = mu + eps*std\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        out = F.relu(self.d1(z))\n",
    "        out = torch.sigmoid(self.d2(out))\n",
    "        return out\n",
    "\n",
    "    def forward(self, x, encoding_only=False, training=True):\n",
    "        mu, logvar = self.encode(x)\n",
    "        if training==True:\n",
    "            z = self.reparametrize(mu, logvar)\n",
    "        else:\n",
    "            z = mu\n",
    "        decoded = self.decode(z)\n",
    "        if encoding_only==False:\n",
    "            return decoded, mu, logvar\n",
    "        else:\n",
    "            return mu\n",
    "        \n",
    "def vae_loss(decoded, x, mu, logvar, beta=1):\n",
    "    loss_r = nn.BCELoss(size_average=False)\n",
    "    l_1 = loss_r(decoded, x)\n",
    "    KLD_element = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar)\n",
    "    l_2 = torch.sum(KLD_element).mul_(-0.5)\n",
    "    return l_1 + beta*l_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ac(object):\n",
    "    def __init__(self, OBSERVATION_SIZE, REPRESENTATION_SIZE, ACTION_SIZE, BATCH_SIZE, DISCOUNT, clip, learning_rate, policy_net, value_net, encoder, entropy_coef):\n",
    "        self.cum_rewards = 0\n",
    "        self.memory = rollouts_ac(BATCH_SIZE, DISCOUNT)\n",
    "        self.policy_net = policy_net\n",
    "        self.value_net = value_net\n",
    "        self.val_loss = torch.nn.L1Loss()\n",
    "        self.OBSERVATION_SIZE = OBSERVATION_SIZE\n",
    "        self.REPRESENTATION_SIZE = REPRESENTATION_SIZE\n",
    "        self.ACTION_SIZE = ACTION_SIZE\n",
    "        self.clip = clip\n",
    "        self.steps = 0\n",
    "        self.o_p = optim.RMSprop(self.policy_net.parameters(), lr=learning_rate)\n",
    "        self.o_v = optim.RMSprop(self.value_net.parameters(), lr=learning_rate)\n",
    "        self.encoder = encoder\n",
    "        self.entropy_coef = entropy_coef\n",
    "      \n",
    "    def entropy(self, p_matrix):\n",
    "        log_probs = torch.log(p_matrix)\n",
    "        entropy = torch.sum(-p_matrix*log_probs, 1)\n",
    "        return torch.mean(entropy)\n",
    "    \n",
    "    def train_networks(self):\n",
    "        state_batch, action_batch, reward_batch, value_batch, new_state_batch, terminal_batch = self.memory.make_a_batch()\n",
    "        # FORWARD\n",
    "        probs = self.policy_net.forward(state_batch)\n",
    "        log_probs = torch.log(probs).gather(1, torch.tensor((action_batch-2), dtype=torch.int64))\n",
    "        critic_values = self.value_net.forward(state_batch)\n",
    "        # TRAIN POLICY\n",
    "        self.o_p.zero_grad()\n",
    "        vs = critic_values.detach()\n",
    "        advantage = value_batch - vs\n",
    "        policy_loss = torch.mean(advantage * -log_probs, dim=0)\n",
    "        #entropy_loss = self.entropy(probs)\n",
    "        loss_p = policy_loss\n",
    "        loss_p.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), self.clip)\n",
    "        self.o_p.step()\n",
    "        # TRAIN VALUE\n",
    "        self.o_v.zero_grad()\n",
    "        loss_v = self.val_loss(critic_values,value_batch)\n",
    "        loss_v.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.value_net.parameters(), self.clip)\n",
    "        self.o_v.step()\n",
    "\n",
    "    def experiment_training(self, env, episodes, scheduler1, scheduler2):\n",
    "        results = []\n",
    "        memory_idx = 18\n",
    "        for i in range(episodes):\n",
    "            observation = env.reset()\n",
    "            self.memory.init_episode()\n",
    "            episode_reward = 0\n",
    "            steps_ = 0\n",
    "            curr_observation = self.preprocess(observation)\n",
    "            curr_observation = torch.tensor(curr_observation, dtype=torch.float32).reshape(1,80*80)\n",
    "            prev_observation1 = torch.zeros(self.REPRESENTATION_SIZE).reshape(1,encoder.latent)\n",
    "            prev_observation2 = torch.zeros(self.REPRESENTATION_SIZE).reshape(1,encoder.latent)\n",
    "            prev_observation3 = torch.zeros(self.REPRESENTATION_SIZE).reshape(1,encoder.latent)\n",
    "            with torch.no_grad():\n",
    "                curr_observation = self.encoder.forward(curr_observation, True, False).detach()\n",
    "            state = torch.cat([curr_observation, prev_observation1, prev_observation2, prev_observation3], 1)\n",
    "            while True:\n",
    "                with torch.no_grad():\n",
    "                    action_probs = self.policy_net.forward(state).detach().numpy()\n",
    "                    action = torch.tensor(np.random.choice(self.ACTION_SIZE, p=action_probs.flatten()), dtype=torch.float32).reshape(1,1) + 2\n",
    "                observation, reward, terminal, _ = env.step(int(action.item()))\n",
    "                episode_reward += reward\n",
    "                steps_ += 1\n",
    "                new_observation = self.preprocess(observation)\n",
    "                new_observation = torch.tensor(new_observation, dtype=torch.float32).reshape(1,80*80)\n",
    "                with torch.no_grad():\n",
    "                    new_observation = self.encoder.forward(new_observation, True, False).detach()\n",
    "                new_state = torch.cat([new_observation, curr_observation, prev_observation1, prev_observation2], 1)\n",
    "                done = torch.zeros(1, dtype=torch.float32) if terminal else torch.ones(1, dtype=torch.float32)\n",
    "                reward = torch.tensor([reward], dtype=torch.float32).reshape(1,1)\n",
    "                if reward == 1 or reward == -1:\n",
    "                    memory_idx = 0\n",
    "                if memory_idx > 18 or memory_idx == 0:\n",
    "                    self.memory.push_to_trajectory(state, action, new_state, reward, done)\n",
    "                if len(self.memory.rollout_memory) > self.memory.batch_size:\n",
    "                    self.train_networks()\n",
    "                    scheduler1.step()\n",
    "                    scheduler2.step()\n",
    "                state = new_state\n",
    "                prev_observation3 = prev_observation2\n",
    "                prev_observation2 = prev_observation1\n",
    "                prev_observation1 = curr_observation\n",
    "                curr_observation = new_observation\n",
    "                memory_idx += 1\n",
    "                if terminal:\n",
    "                    results.append(episode_reward)\n",
    "                    self.cum_rewards += episode_reward\n",
    "                    self.memory.push_to_memory_unfinished(self.value_net)\n",
    "                    print(\"\\rEp: {} Online reward: {:.2f}; Steps: {}\".format(i + 1, episode_reward, steps_), end=\"\")\n",
    "                    break\n",
    "        return results\n",
    "    \n",
    "    def preprocess(self, state):\n",
    "        state = state[35:195]\n",
    "        state = state[::2,::2,0]\n",
    "        state[state == 144] = 0\n",
    "        state[state == 109] = 0\n",
    "        state[state != 0] = 1\n",
    "        state = self.enlarge_ball(state)\n",
    "        return state.astype(np.float).ravel()\n",
    "    \n",
    "    def enlarge_ball(self, image):\n",
    "        image = np.copy(image)\n",
    "        for i in range(1,79):\n",
    "            for j in range(1,79):\n",
    "                if image[i,j]==1:\n",
    "                    if image[i,j+1]==0 and image[i,j-1]==0:\n",
    "                        if image[i+1,j]==1:\n",
    "                            image[i,j-1] = 1\n",
    "                            image[i+1,j-1] = 1\n",
    "                            image[i,j+1] = 1\n",
    "                            image[i+1,j+1] = 1\n",
    "                            if i!=0:\n",
    "                                image[i-1,j-1:j+2] = 1\n",
    "                                if i!=1:\n",
    "                                    image[i-2,j-1:j+2] = 1\n",
    "                            if i!=78:\n",
    "                                image[i+2,j-1:j+2] = 1\n",
    "                                if i!=77:\n",
    "                                    image[i+3,j-1:j+2] = 1\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 2 Online reward: -21.00; Steps: 764"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 4 Online reward: -21.00; Steps: 7642"
     ]
    }
   ],
   "source": [
    "SEED = 6\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "encoder = VAE(6400, 512, 16)\n",
    "encoder.load_state_dict(torch.load('VAE_MLP_512_16'))\n",
    "\n",
    "env = gym.make(\"PongDeterministic-v4\")\n",
    "env.frameskip = 4\n",
    "LATENT = 16\n",
    "HIDDEN_AC = [512]\n",
    "EPOCHS_VAE = 50\n",
    "EPISODES_TO_GATHER = 10\n",
    "BATCH_SIZE = 512\n",
    "OBSERVATION_SIZE = 6400\n",
    "ACTION_SIZE = 2\n",
    "LR = 0.001\n",
    "EPISODES_AC = 1000\n",
    "ENTROPY_COEF = 0.01\n",
    "\n",
    "policy_net = deterministic_policy(4*LATENT, HIDDEN_AC, ACTION_SIZE)\n",
    "value_net = neural_net(4*LATENT, HIDDEN_AC, 1)\n",
    "ac_agent = ac(OBSERVATION_SIZE, LATENT, ACTION_SIZE, BATCH_SIZE, 0.99, 1, LR, policy_net, value_net, encoder, ENTROPY_COEF)\n",
    "scheduler1 = optim.lr_scheduler.StepLR(ac_agent.o_p, step_size=10, gamma=0.99)\n",
    "scheduler2 = optim.lr_scheduler.StepLR(ac_agent.o_v, step_size=10, gamma=0.99)\n",
    "results = ac_agent.experiment_training(env, EPISODES_AC, scheduler1, scheduler2)\n",
    "results = np.array(results)\n",
    "np.savetxt('AC_pong1.csv', results, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m55",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m55"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
