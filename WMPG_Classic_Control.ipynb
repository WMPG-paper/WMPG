{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class policy_network(nn.Module):\n",
    "    # MLP softmax output\n",
    "    def __init__(self, state_size, hidden_list, action_size):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        prev_layer = state_size\n",
    "        for layer in hidden_list:\n",
    "            self.layers.append(nn.Linear(prev_layer, layer))\n",
    "            self.layers.append(nn.ReLU())\n",
    "            prev_layer = layer\n",
    "        self.layers.append(nn.Linear(prev_layer, action_size))                      \n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return F.softmax(x, dim=-1)\n",
    "        \n",
    "class value_network(nn.Module):\n",
    "    # MLP no softmax on output\n",
    "    def __init__(self, state_size, hidden_list, output_size):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        prev_layer = state_size\n",
    "        for layer in hidden_list:\n",
    "            self.layers.append(nn.Linear(prev_layer, layer))\n",
    "            self.layers.append(nn.ReLU())\n",
    "            prev_layer = layer\n",
    "        self.layers.append(nn.Linear(prev_layer, output_size))     \n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "class transition_network(nn.Module):\n",
    "    # MLP with forward conditioned on action; output of size of input\n",
    "    def __init__(self, state_size, hidden_list, action_size):\n",
    "        super().__init__()\n",
    "        self.action_dim = action_size\n",
    "        self.layers = nn.ModuleList()\n",
    "        prev_layer = state_size\n",
    "        for idx, layer in enumerate(hidden_list):\n",
    "            if idx == 0:\n",
    "                if action_size == 2:\n",
    "                    self.layers.append(nn.Linear(prev_layer+1, layer))\n",
    "                else:\n",
    "                    self.layers.append(nn.Linear(prev_layer+action_size, layer))\n",
    "            if idx > 0:\n",
    "                self.layers.append(nn.Linear(prev_layer, layer))\n",
    "            self.layers.append(nn.ReLU())\n",
    "            prev_layer = layer\n",
    "        self.layers.append(nn.Linear(prev_layer, state_size))\n",
    "        \n",
    "    def forward(self, x, a):\n",
    "        if self.action_dim == 2:\n",
    "            out = torch.cat((x, a), 1)\n",
    "        if self.action_dim != 2:\n",
    "            extra = torch.zeros([x.size(0), self.action_dim], dtype=torch.float32)\n",
    "            a_ = torch.tensor(a, dtype=torch.int64)\n",
    "            extra = extra.scatter(1,a_,1)\n",
    "            out = torch.cat((x, extra), 1)\n",
    "        for layer in self.layers:\n",
    "            out = layer(out)\n",
    "        return out\n",
    "\n",
    "class reward_network(nn.Module):\n",
    "    # MLP with forward conditioned on action; output of size 1\n",
    "    def __init__(self, state_size, hidden_list, action_size):\n",
    "        super().__init__()\n",
    "        self.action_dim = action_size\n",
    "        self.layers = nn.ModuleList()\n",
    "        prev_layer = state_size\n",
    "        for idx, layer in enumerate(hidden_list):\n",
    "            if idx == 0:\n",
    "                if action_size == 2:\n",
    "                    self.layers.append(nn.Linear(prev_layer+1, layer))\n",
    "                else:\n",
    "                    self.layers.append(nn.Linear(prev_layer+action_size, layer))\n",
    "            if idx > 0:\n",
    "                self.layers.append(nn.Linear(prev_layer, layer))\n",
    "            self.layers.append(nn.ReLU())\n",
    "            prev_layer = layer\n",
    "        self.layers.append(nn.Linear(prev_layer, 1))   \n",
    "        \n",
    "    def forward(self, x, a):\n",
    "        if self.action_dim == 2:\n",
    "            x = torch.cat((x, a), 1)\n",
    "        else:\n",
    "            extra = torch.zeros([x.size(0), self.action_dim], dtype=torch.float32)\n",
    "            a_ = torch.tensor(a, dtype=torch.int64)\n",
    "            extra = extra.scatter(1,a_,1)\n",
    "            x = torch.cat((x, extra), 1)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, state, hidden, latent):\n",
    "        super(VAE, self).__init__()\n",
    "        self.e1 = nn.Linear(state, hidden)\n",
    "        self.e21 = nn.Linear(hidden, latent)\n",
    "        self.e22 = nn.Linear(hidden, latent)\n",
    "        self.d1 = nn.Linear(latent, hidden)\n",
    "        self.d2 = nn.Linear(hidden, state)\n",
    "        self.latent = latent\n",
    "\n",
    "    def encode(self, x):\n",
    "        out = F.relu(self.e1(x))\n",
    "        mu = self.e21(out)\n",
    "        logvar = self.e22(out)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparametrize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        eps = torch.FloatTensor(mu.size()).normal_()\n",
    "        z = mu + eps*std\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        out = F.relu(self.d1(z))\n",
    "        out = torch.sigmoid(self.d2(out))\n",
    "        return out\n",
    "\n",
    "    def forward(self, x, encoding_only=False, training=True):\n",
    "        mu, logvar = self.encode(x)\n",
    "        if training==True:\n",
    "            z = self.reparametrize(mu, logvar)\n",
    "        else:\n",
    "            z = mu\n",
    "        decoded = self.decode(z)\n",
    "        if encoding_only==False:\n",
    "            return decoded, mu, logvar\n",
    "        else:\n",
    "            return mu\n",
    "        \n",
    "def vae_loss(decoded, x, mu, logvar, beta=1):\n",
    "    loss_r = nn.BCELoss(size_average=False)\n",
    "    l_1 = loss_r(decoded, x)\n",
    "    KLD_element = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar)\n",
    "    l_2 = torch.sum(KLD_element).mul_(-0.5)\n",
    "    return l_1 + beta*l_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rollouts_wm(object):\n",
    "    def __init__(self, batch_size, discount, experience_lenght):\n",
    "        self.batch_size = batch_size\n",
    "        self.rollout_memory = []\n",
    "        self.rollout_values = []\n",
    "        self.batch = []\n",
    "        self.discount = discount\n",
    "        self.transition = namedtuple('transition', ('state', 'action', 'next_state', 'reward', 'terminal'))\n",
    "        self.experience_buffer = []\n",
    "        self.experience_lenght = experience_lenght\n",
    "\n",
    "    def init_episode(self):\n",
    "        self.trajectory = []\n",
    "        \n",
    "    def push_to_trajectory(self, state, action, next_state, reward, terminal):\n",
    "        self.trajectory.append(self.transition(state, action, next_state, reward, terminal))\n",
    "    \n",
    "    def monte_carlo(self, value_network=None):\n",
    "        values = []\n",
    "        if value_network is None:\n",
    "            for idx, state in enumerate(self.trajectory):\n",
    "                value = 0 \n",
    "                for idx_, state_ in enumerate(self.trajectory[idx:]):\n",
    "                    value += self.discount**idx_ * state_.reward\n",
    "                values.append(value)\n",
    "        else:\n",
    "            final_state = self.trajectory[-1].next_state\n",
    "            with torch.no_grad():\n",
    "                final_state_val = value_network.forward(final_state)\n",
    "            for idx, state in enumerate(self.trajectory):\n",
    "                value = 0 \n",
    "                for idx_, state_ in enumerate(self.trajectory[idx:]):\n",
    "                    value += self.discount**idx_ * state_.reward\n",
    "                    if (idx + idx_ + 1) == len(self.trajectory):\n",
    "                        value += self.discount**(idx_+1) * final_state_val\n",
    "                values.append(value)\n",
    "        return values\n",
    "    \n",
    "    def push_to_memory(self, value_network=None):\n",
    "        if value_network is None:\n",
    "            values = self.monte_carlo()\n",
    "        else:\n",
    "            values = self.monte_carlo(value_network)\n",
    "        self.rollout_memory = self.rollout_memory + self.trajectory\n",
    "        self.rollout_values = self.rollout_values + values\n",
    "        self.experience_buffer = self.experience_buffer + self.trajectory\n",
    "        if len(self.experience_buffer) > self.experience_lenght:\n",
    "            del self.experience_buffer[:(len(self.experience_buffer)-self.experience_lenght+1)]              \n",
    "\n",
    "    def sample_data(self, policy_mode=True):\n",
    "        if policy_mode is True:\n",
    "            batch = self.rollout_memory[:self.batch_size]\n",
    "            batch = self.transition(*zip(*batch))\n",
    "            self.rollout_memory = self.rollout_memory[self.batch_size:]\n",
    "            values = torch.cat(self.rollout_values[:self.batch_size])\n",
    "            self.rollout_values = self.rollout_values[self.batch_size:]\n",
    "            state = torch.cat(batch.state)\n",
    "            return state, values\n",
    "        if policy_mode is False:\n",
    "            sample_ = random.sample(self.experience_buffer, self.batch_size)\n",
    "            batch = self.transition(*zip(*sample_))\n",
    "            #terminal = torch.cat(batch.terminal)\n",
    "            state = torch.cat(batch.state)\n",
    "            action = torch.cat(batch.action)\n",
    "            reward = torch.cat(batch.reward)\n",
    "            #reward = reward * terminal\n",
    "            new_state = torch.cat(batch.next_state)\n",
    "            return state, action, reward, new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class wmpg(object):\n",
    "    def __init__(self, state_size, action_size, batch_size, discount, policy_net, value_net, transition_net, reward_net, \n",
    "                 value_updates=3, policy_updates=3, wm_updates=3, imagination_horizon=5, imagination_lambda=0.75,\n",
    "                 wm_memory=2000, policy_lr=0.0025, value_lr=0.0025, transition_lr=0.005, reward_lr=0.005, clip=None, \n",
    "                 scheduler_step=None, termination_length=None, k=None):\n",
    "        self.memory = rollouts_wm(batch_size, discount, wm_memory)\n",
    "        self.policy_net = policy_net\n",
    "        self.value_net = value_net\n",
    "        self.transition_net = transition_net\n",
    "        self.reward_net = reward_net\n",
    "        self.o_p = optim.RMSprop(self.policy_net.parameters(), lr=policy_lr, eps=1e-5)\n",
    "        self.o_v = optim.RMSprop(self.value_net.parameters(), lr=value_lr, eps=1e-5)\n",
    "        self.o_t = optim.Adam(self.transition_net.parameters(), lr=transition_lr)\n",
    "        self.o_r = optim.Adam(self.reward_net.parameters(), lr=reward_lr)\n",
    "        self.loss_v = torch.nn.L1Loss()\n",
    "        self.loss_t = torch.nn.L1Loss()\n",
    "        self.loss_r = torch.nn.L1Loss()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.batch_size = batch_size\n",
    "        self.discount = discount\n",
    "        self.value_updates = int(value_updates)\n",
    "        self.wm_updates = int(wm_updates)\n",
    "        self.policy_updates = int(policy_updates)\n",
    "        self.imagination_horizon = imagination_horizon\n",
    "        self.imagination_lambda = imagination_lambda\n",
    "        self.cum_rewards = 0\n",
    "        self.clip = clip\n",
    "        if scheduler_step is not None:\n",
    "            self.scheduler_t = optim.lr_scheduler.StepLR(self.o_t, step_size=scheduler_step, gamma=0.99)\n",
    "            self.scheduler_r = optim.lr_scheduler.StepLR(self.o_r, step_size=scheduler_step, gamma=0.99)\n",
    "            self.scheduler_v = optim.lr_scheduler.StepLR(self.o_v, step_size=scheduler_step, gamma=0.99)\n",
    "            self.scheduler_p = optim.lr_scheduler.StepLR(self.o_p, step_size=scheduler_step, gamma=0.99)\n",
    "        self.scheduler_step = scheduler_step\n",
    "        self.termination_length = termination_length\n",
    "        self.k = k\n",
    "        \n",
    "    def train_wm(self):\n",
    "        for i in range(self.wm_updates):\n",
    "            states, actions, rewards, new_states = self.memory.sample_data(False)\n",
    "            self.o_t.zero_grad()\n",
    "            transitions = self.transition_net.forward(states, actions)\n",
    "            loss_t_ = self.loss_t(transitions, new_states)\n",
    "            loss_t_.backward()\n",
    "            self.o_t.step()\n",
    "            self.o_r.zero_grad()\n",
    "            rewards_ = self.reward_net.forward(states, actions)\n",
    "            loss_r_ = self.loss_r(rewards_, rewards)\n",
    "            loss_r_.backward()\n",
    "            self.o_r.step()\n",
    "            \n",
    "    def train_value(self, states, values):\n",
    "        for i in range(self.value_updates):\n",
    "            self.o_v.zero_grad()\n",
    "            values_ = self.value_net.forward(states)\n",
    "            loss_v_ = self.loss_v(values_, values)\n",
    "            loss_v_.backward()\n",
    "            if self.clip is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(self.value_net.parameters(), self.clip)\n",
    "            self.o_v.step()\n",
    "    \n",
    "    def train_policy(self, states):\n",
    "        self.o_p.zero_grad()\n",
    "        probabilities = self.policy_net.forward(states)\n",
    "        q_values = self.imagine_values(states)\n",
    "        if self.imagination_horizon != 1:\n",
    "            baseline = torch.sum(q_values * probabilities.detach(), dim=1).unsqueeze(-1)\n",
    "            expected_values = torch.sum((q_values - baseline) * probabilities, dim=1)\n",
    "        if self.imagination_horizon == 1:\n",
    "            expected_values = torch.sum(q_values * probabilities, dim=1)\n",
    "        loss_pg = -torch.mean(expected_values, dim=0)\n",
    "        loss_pg.backward()\n",
    "        if self.clip is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), self.clip)\n",
    "        self.o_p.step()\n",
    "            \n",
    "    def schedulers_step(self):\n",
    "        if self.scheduler_step is not None:\n",
    "            self.scheduler_t.step()\n",
    "            self.scheduler_r.step()\n",
    "            self.scheduler_v.step()\n",
    "            self.scheduler_p.step()\n",
    "        \n",
    "    def train_networks(self):\n",
    "        state_batch, value_batch = self.memory.sample_data()\n",
    "        for i_g in range(self.policy_updates):    \n",
    "            self.train_wm()\n",
    "            self.train_value(state_batch, value_batch)\n",
    "            if self.k is None:\n",
    "                self.train_policy(state_batch)\n",
    "            if self.k == 1:\n",
    "                self.train_policy_1(state_batch)\n",
    "        self.schedulers_step()\n",
    "            \n",
    "    def imagine_values(self, states):\n",
    "        q_values = torch.zeros((self.batch_size, self.action_size), dtype=torch.float32)\n",
    "        if self.imagination_lambda == 1:\n",
    "            for i in range(self.action_size):\n",
    "                transitions = states\n",
    "                actions = torch.zeros([self.batch_size, 1], dtype=torch.float32)+i\n",
    "                running_rewards = torch.zeros([self.batch_size, 1], dtype=torch.float32)\n",
    "                with torch.no_grad():\n",
    "                    for j in range(self.imagination_horizon):\n",
    "                        reward = self.reward_net.forward(transitions, actions)\n",
    "                        running_rewards += self.discount**j * reward\n",
    "                        transitions = self.transition_net.forward(transitions, actions)\n",
    "                        probabilities = self.policy_net.forward(transitions).detach().numpy()\n",
    "                        actions = self.sample_from_matrix(probabilities)\n",
    "                    running_rewards += self.discount**(self.imagination_horizon) * self.value_net.forward(transitions)\n",
    "                q_values[:,i] = running_rewards.squeeze()\n",
    "        if self.imagination_lambda != 1:\n",
    "            for i in range(self.action_size):\n",
    "                transitions = states\n",
    "                actions = torch.zeros([self.batch_size, 1], dtype=torch.float32)+i\n",
    "                running_rewards = torch.zeros([self.batch_size, 1], dtype=torch.float32)\n",
    "                running_lambdas = torch.zeros([self.batch_size, 1], dtype=torch.float32)\n",
    "                with torch.no_grad():\n",
    "                    for j in range(self.imagination_horizon):\n",
    "                        rewards = self.reward_net.forward(transitions, actions)\n",
    "                        running_rewards += self.discount**j * rewards\n",
    "                        transitions = self.transition_net.forward(transitions, actions)\n",
    "                        values = running_rewards + self.discount**(j+1) * self.value_net.forward(transitions)\n",
    "                        if (j+1) == self.imagination_horizon:\n",
    "                            running_lambdas += self.imagination_lambda**(j)*values\n",
    "                        else:\n",
    "                            running_lambdas += (1 - self.imagination_lambda)*self.imagination_lambda**j*values\n",
    "                        probabilities = self.policy_net.forward(transitions).detach().numpy()\n",
    "                        actions = self.sample_from_matrix(probabilities)\n",
    "                q_values[:,i] = running_lambdas.squeeze()\n",
    "        return q_values\n",
    "    \n",
    "    def train_policy_1(self, states):\n",
    "        self.o_p.zero_grad()\n",
    "        probabilities = self.policy_net.forward(states)\n",
    "        actions = self.sample_from_matrix(probabilities.detach().numpy())\n",
    "        log_probabilities = torch.log(probabilities.gather(1, torch.tensor(actions, dtype=torch.int64)))\n",
    "        q_values = self.imagine_values_1(states, actions)\n",
    "        with torch.no_grad():\n",
    "            baseline = self.value_net.forward(states).detach()\n",
    "        expected_values = torch.sum((q_values - baseline) * log_probabilities, dim=1)\n",
    "        loss_pg = -torch.mean(expected_values, dim=0)\n",
    "        loss_pg.backward()\n",
    "        if self.clip is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), self.clip)\n",
    "        self.o_p.step()\n",
    "        \n",
    "    def imagine_values_1(self, states, actions):\n",
    "        q_values = torch.zeros((self.batch_size, 1), dtype=torch.float32)\n",
    "        if self.imagination_lambda == 1:\n",
    "            transitions = states\n",
    "            running_rewards = torch.zeros([self.batch_size, 1], dtype=torch.float32)\n",
    "            with torch.no_grad():\n",
    "                for j in range(self.imagination_horizon):\n",
    "                    reward = self.reward_net.forward(transitions, actions)\n",
    "                    running_rewards += self.discount**j * reward\n",
    "                    transitions = self.transition_net.forward(transitions, actions)\n",
    "                    probabilities = self.policy_net.forward(transitions).detach().numpy()\n",
    "                    actions = self.sample_from_matrix(probabilities)\n",
    "                running_rewards += self.discount**(self.imagination_horizon) * self.value_net.forward(transitions)\n",
    "            q_values[:,0] = running_rewards.squeeze()\n",
    "        if self.imagination_lambda != 1:\n",
    "            transitions = states\n",
    "            running_rewards = torch.zeros([self.batch_size, 1], dtype=torch.float32)\n",
    "            running_lambdas = torch.zeros([self.batch_size, 1], dtype=torch.float32)\n",
    "            with torch.no_grad():\n",
    "                for j in range(self.imagination_horizon):\n",
    "                    rewards = self.reward_net.forward(transitions, actions)\n",
    "                    running_rewards += self.discount**j * rewards\n",
    "                    transitions = self.transition_net.forward(transitions, actions)\n",
    "                    values = running_rewards + self.discount**(j+1) * self.value_net.forward(transitions)\n",
    "                    if (j+1) == self.imagination_horizon:\n",
    "                        running_lambdas += self.imagination_lambda**(j)*values\n",
    "                    else:\n",
    "                        running_lambdas += (1 - self.imagination_lambda)*self.imagination_lambda**j*values\n",
    "                    probabilities = self.policy_net.forward(transitions).detach().numpy()\n",
    "                    actions = self.sample_from_matrix(probabilities)\n",
    "            q_values[:,0] = running_lambdas.squeeze()\n",
    "        return q_values\n",
    "                \n",
    "    def sample_from_matrix(self, probs):\n",
    "        cumulative = probs.cumsum(axis=1)\n",
    "        uniform_samples = np.random.rand(len(cumulative), 1)\n",
    "        samples = (uniform_samples < cumulative).argmax(axis=1).astype(np.float32)\n",
    "        samples = torch.tensor(samples.reshape(np.shape(samples)[0],1))\n",
    "        return samples.float()\n",
    "        \n",
    "    def training(self, env, episodes):\n",
    "        results = np.zeros(episodes)\n",
    "        for i in range(episodes):\n",
    "            state = env.reset()\n",
    "            self.memory.init_episode()\n",
    "            episode_reward = 0\n",
    "            steps = 0\n",
    "            state = torch.tensor(state, dtype=torch.float32).reshape(1, self.state_size)\n",
    "            while True:\n",
    "                with torch.no_grad():\n",
    "                    probabilities = self.policy_net.forward(state).detach().numpy()\n",
    "                    action = torch.tensor(np.random.choice(self.action_size, p=probabilities.flatten()), dtype=torch.float32).reshape(1,1)\n",
    "                new_state, reward, terminal, _ = env.step(int(action.item()))\n",
    "                episode_reward += reward\n",
    "                steps += 1       \n",
    "                done = torch.zeros([1, 1], dtype=torch.float32) if terminal else torch.ones([1, 1], dtype=torch.float32)\n",
    "                reward = torch.zeros([1,1], dtype=torch.float32) + reward\n",
    "                new_state = torch.tensor(new_state, dtype=torch.float32).reshape(1, self.state_size)\n",
    "                self.memory.push_to_trajectory(state, action, new_state, reward, done)\n",
    "                if len(self.memory.rollout_memory) > self.memory.batch_size:\n",
    "                    self.train_networks()\n",
    "                state = new_state\n",
    "                if terminal:\n",
    "                    results[i] = episode_reward\n",
    "                    if self.termination_length is not None:\n",
    "                        if steps < self.termination_length:\n",
    "                            self.memory.push_to_memory()\n",
    "                        else:\n",
    "                            self.memory.push_to_memory(self.value_net)\n",
    "                    if self.termination_length is None:\n",
    "                        self.memory.push_to_memory()\n",
    "                    print(\"\\rEp: {} Online reward: {:.2f}\".format(i+1, episode_reward), end=\"\")\n",
    "                    break\n",
    "        return np.array(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 131 Online reward: 200.00"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-32866ffecdcc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m agent = wmpg(state_size, action_size, batch_size, discount, policy_net, value_net, transition_net, reward_net, i_v, \n\u001b[1;32m     35\u001b[0m              i_g, i_wm, horizon, lambda_, 5000, lr_p, lr_v, lr_t, lr_r, clip, step, termination_length, k)\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-19ac2f453c89>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(self, env, episodes)\u001b[0m\n\u001b[1;32m    205\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_to_trajectory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollout_memory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_networks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m                 \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mterminal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-19ac2f453c89>\u001b[0m in \u001b[0;36mtrain_networks\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_policy_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-19ac2f453c89>\u001b[0m in \u001b[0;36mtrain_policy\u001b[0;34m(self, states)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mo_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mprobabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimagine_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimagination_horizon\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mbaseline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_values\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mprobabilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-19ac2f453c89>\u001b[0m in \u001b[0;36mimagine_values\u001b[0;34m(self, states)\u001b[0m\n\u001b[1;32m    125\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                             \u001b[0mrunning_lambdas\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimagination_lambda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimagination_lambda\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m                         \u001b[0mprobabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m                         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_from_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobabilities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                 \u001b[0mq_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunning_lambdas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-2cfb0bd40e53>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1608\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1609\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1610\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1611\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1612\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# CartPole\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "SEED = 10\n",
    "batch_size = 32\n",
    "discount = 0.99\n",
    "hidden_p = [32]\n",
    "hidden_v = [32]\n",
    "hidden_t = [64]\n",
    "hidden_r = [64]\n",
    "i_v = 3\n",
    "i_g = 5\n",
    "i_wm = 5\n",
    "horizon = 15\n",
    "lambda_ = 0.75\n",
    "lr_p = 0.0025\n",
    "lr_v = 0.0025\n",
    "lr_t = 0.005\n",
    "lr_r = 0.005\n",
    "episodes = 200\n",
    "clip = 10\n",
    "step = 10\n",
    "termination_length = 199\n",
    "k = None\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "policy_net = policy_network(state_size, hidden_p, action_size)\n",
    "value_net = value_network(state_size, hidden_v, 1)\n",
    "reward_net = reward_network(state_size, hidden_r, action_size)\n",
    "transition_net = transition_network(state_size, hidden_t, action_size)\n",
    "agent = wmpg(state_size, action_size, batch_size, discount, policy_net, value_net, transition_net, reward_net, i_v, \n",
    "             i_g, i_wm, horizon, lambda_, 5000, lr_p, lr_v, lr_t, lr_r, clip, step, termination_length, k)\n",
    "results = agent.training(env, episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Ep: 1 Online reward: -97.24\r",
      "Ep: 2 Online reward: -394.75"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:90: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 153 Online reward: 239.784"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-88a993e3f5dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m agent = wmpg(state_size, action_size, batch_size, discount, policy_net, value_net, transition_net, reward_net, i_v, \n\u001b[1;32m     35\u001b[0m              i_g, i_wm, horizon, lambda_, 5000, lr_p, lr_v, lr_t, lr_r, clip, step, termination_length, k)\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-19ac2f453c89>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(self, env, episodes)\u001b[0m\n\u001b[1;32m    211\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtermination_length\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtermination_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_to_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_to_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_net\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-909077052b9a>\u001b[0m in \u001b[0;36mpush_to_memory\u001b[0;34m(self, value_network)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpush_to_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_network\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalue_network\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonte_carlo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonte_carlo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue_network\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-909077052b9a>\u001b[0m in \u001b[0;36mmonte_carlo\u001b[0;34m(self, value_network)\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0midx_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrajectory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                     \u001b[0mvalue\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscount\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0midx_\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstate_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m                 \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#LunarLander\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "SEED = 10\n",
    "batch_size = 64\n",
    "discount = 0.99\n",
    "hidden_p = [64]\n",
    "hidden_v = [64]\n",
    "hidden_t = [96]\n",
    "hidden_r = [96]\n",
    "i_v = 3\n",
    "i_g = 4\n",
    "i_wm = 5\n",
    "horizon = 3\n",
    "lambda_ = 0.75\n",
    "lr_p = 0.0025\n",
    "lr_v = 0.005\n",
    "lr_t = 0.005\n",
    "lr_r = 0.005\n",
    "episodes = 1000\n",
    "clip = 10\n",
    "step = 10\n",
    "termination_length = 10000\n",
    "k = None\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "policy_net = policy_network(state_size, hidden_p, action_size)\n",
    "value_net = value_network(state_size, hidden_v, 1)\n",
    "reward_net = reward_network(state_size, hidden_r, action_size)\n",
    "transition_net = transition_network(state_size, hidden_t, action_size)\n",
    "agent = wmpg(state_size, action_size, batch_size, discount, policy_net, value_net, transition_net, reward_net, i_v, \n",
    "             i_g, i_wm, horizon, lambda_, 5000, lr_p, lr_v, lr_t, lr_r, clip, step, termination_length, k)\n",
    "results = agent.training(env, episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m55",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m55"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
